{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "skilled-collector",
   "metadata": {},
   "source": [
    "### Resúmenes del trabajo semanal\n",
    "##### Elaborado por: Jorge Eduardo Gutiérrez Mata\n",
    "\n",
    "### Introducción al Reinforcement Learning\n",
    "#### Definiciones\n",
    "1. **IA:** El área que estudia máquina que están alcanzando un nivel de rendimiento humano en una tarea específica, ya sea reconocer spam, fraudes, rostros. En palabras técnicas es el campo que estudia agentes inteligentes que perciben su ambiante y realizan acciones con la finalidad de resolver una tarea.\n",
    "\n",
    "1. **Machine Learning:** Son todos los algoritmos que son capaces de enseñar a la computadora una tarea humana por medio de la experiencia con la finalidad de aumentar el rendimiento gracias a esta.\n",
    "\n",
    "##### **Tipos de tareas**\n",
    "Percepción es la que se refiera a clasificar correctamente (como perros y gatos)\n",
    "Acción son los tipos de tareas que requieren multiples acciones que se dividen en subtareas, como el tener un vehiculo autónomo que defina una ruta para moverse del punto A al B. Estas son las tareas de acción\n",
    "\n",
    "El rendimiento es específico de la tarea que queramos solucionar, es decir depende al área en la que estemos. Usando ejemplos reales para el vehículo autónomo pone el tiempo que dura en llegar, el combustible que gasta, entre otros.\n",
    "\n",
    "La experiencia le da al algoritmo la habilidad de generalizar, gracias a ella podemos dividir los algoritmos en supervisado, no supervisado y el aprendizaje por refuerzo.\n",
    "\n",
    "#### Paradigmas\n",
    "Como hemos visto en el curso, el aprendizaje supervisado es aquel que maneja problemas de clasificación y de regresión, estos buscan predecir un número, una etiqueta o un valor booleano. Pone como ejemplo obtener a futuro los ingresos de una empresa así cómo clasificar un caso de cancer y no cancer.\n",
    "\n",
    "Lo que es el aprendizaje no supervisado se divide en Clustering (agrupamiento) como por ejemplo un algoritmo que detecte fraudes que se realizan con tarjetas de crédito, para ello busca anomalías o patrones fuera de lo común.\n",
    "\n",
    "El aprendizaje reforzado (RL) es un híbrido entre ambos, hay una etapa de entranamiento pero no se le dice el resultado correcto, se le dan recompenzas dependiendo de las acciones que vaya realizando, así por si solo se da cuenta de cuales acciones le permiten optener una mayor recompensa. Se usan cuando queremos solucionar las **tareas de acción** como para saber cuando comprar y vender acciones.\n",
    "\n",
    "#### RL History\n",
    "1. Nace con Edward Thorndike con la ley de causa y efecto y la ley de prueba y error. (Los animales aprenden asociando con recompensas).\n",
    "\n",
    "1. Richard Bellman con la teoría de control óptimo, la programación dinámica y la ecuación de Bellman.\n",
    "\n",
    "1. Chris Watkins habló sobre la diferencia temporal y el algoritmo Q learning.\n",
    "\n",
    "1. Se habla de Deep mind y alpha GO, también crearon alphaFold que es capáz de producir la síntesis de una estructuras de proteínas (tarea muy compleja).\n",
    "\n",
    "#### Conceptos básicos de RL\n",
    "1. Ambiente: el mundo en el que vive la IA. EJ: el vehículo autónomo viaja por la ruta 27 de San José a Caldera.\n",
    "\n",
    "1. Agente: Es la IA tratando de solucionar la tarea, osea el vehículo autónomo.\n",
    "\n",
    "1. Las acciones: decisiones que toma el agente para solucionar la terea como acelerar, frenar, etc.\n",
    "\n",
    "1. Esados: configuraciones del ambiente, en este caso el saber que hay un peaje con 6 carriles, pasar por túneles, cambio de tipo de carretera.\n",
    "\n",
    "1. Recompensa: Son números enteros (-,+).\n",
    "\n",
    "1. Policy: Es el criterio que sua el IA para escoger las acciones. Ej: llegar a Caldera en el menor tiempo posible, entonces él vehículo tendría que preveer las acciones para cumplir con el objetivo\n",
    "\n",
    "Todo esto ayuda a conseguir la meta del RL, es decir, maximizar la recompensa total.\n",
    "\n",
    "##### Proceso estocástico\n",
    "Es un proceso que implica cierta aleatoriedad en el ambiente, como ejemplo pone un dron que intenta desplegar un líquido para apagar un incendio, aquí damos por un hecho que no siempre que el dron quiera cumplir con su objetivo lo va a lograr.\n",
    "\n",
    "##### Proceso de desición de Markov\n",
    "La probabilidad de que se desplaze el drón depende del estado actual. No se modela con momentos anteriores.\n",
    "\n",
    "1. Tareas episódica: aquellas que se pueden repetir como las partidas de ajedréz.\n",
    "1. Tareas continuas: son la que no se reinician como una IA que controle la temperatura en el edificio.\n",
    "\n",
    "#### Ecuación de Bellman (Intuición)\n",
    "Perminte relacionar el valor del estado actual con el de futuros estados sin tener que esperar para observar las futuras recompensas.\n",
    "\n",
    "Aquí ponen como ejemplo un agente que moviliza cajas en bodegas de Amazón. El agente se mueve en una matríz, hay casillas que tienen valores positivos y negativos según el cumplimiento de la meta del agente (estado final o estado por evitar) y casillas que no tienen valores con la finalidad de que el agente llega a su meta.\n",
    "\n",
    "Nos comenta que inicialmente el robotcito no tiene criterio para saber a dónde debería ir, para ello se le coloca un Policy aleatorio; este se va modificando conforme se vayan realizando más pruebas con las funciones de valor.\n",
    "\n",
    "La idea es irse moviendo entre estos estados, si al moverse el estado no tiene ningún valor, a la casilla de donde venía, se le asigna un 0, en cambio, si al moverse a una casilla llega a un estado positivo (1) entonces a la casilla anterior se le agraga el 1. Lo mismo ocurre con los valores negativos, esto se llama **retropropagación.**\n",
    "\n",
    "Es gracias a este factor que en la siguiente iteración del algoritmo se conozcan todos los valores a los que puede llegar, incluyendo por el obtiene la recompensa, el detalle es que no se puede asegurar que se va a tomar siempre el camino correcto. \n",
    "\n",
    "Para ello surge el concepto del **Factor de descuento** el cual ayuda a disminuir la insertidumbre en la desición. Este le resta un poco a las recompensas que se van estimando en el futuro, dándole peso así a la decisión imnediata.\n",
    "\n",
    "También se agrega el **Valor para estado acción** con la finalidad de realizar un mejor cálculo del camino, para ello se le dice al agente que seleccione la acción que le permita maximizar el valor de \"q\" siendo este un conjunto \"Estado-acción\".\n",
    "Esto se realiza con un calculo entre la sumatoria de las razompensas al escoger x acción entre el número de veces que se seleccionó esa acción.\n",
    "\n",
    "Para terminar de asegurar la funcionalidad del algoritmo se le agrega un factor de aleatoriedad en cuanto a que se logre la acción que le corresponde ejecutar, esto porque el ambiente real es estocástico. Luego de eso se puede lograr generar un modelo que conozca exactamente por donde se debe movilizar.\n",
    "\n",
    "Es importante resaltar que incluso aunque el robotcito ya conozca la ruta adecuada para llegar a las bodegas se debe dejar la posibilidad de que este siga explorando, pues puede que haya algún cambio en el ambiente, esto se raliza con la fórmula Epsilon-Greedy donde uno puede indicarle que en un pequeño % de veces no seleccione la acción \"correcta\" sino una al azar.\n",
    "\n",
    "**Fin del primer resúmen**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-cooperation",
   "metadata": {},
   "source": [
    "### Adversarial Search\n",
    "PD: profe, me encantó este vídeo, me sentí en Estructuras de datos otra vez combinado con IA, fue asombroso. Estaría contento de trabajar en un proyecto como el de Pacman con turnos, se ve interesante. Ahora si, comienzo el resúmen...\n",
    "\n",
    "1. La primera vez que se usó IA para intentar ganar un juego fue con Damas cerca de 1950 donde simplemente era un rival más. Hasta 1994 se coronó la primera computadora campeona en Damas, se llamaba Chinook, venció al campeón mundial Marion Tinsley usando las 8 piesas del juego. Así que para el 2007 se consideró el juego de damos como \"resuelto\" es decir que no se puede resolver mejor.\n",
    "\n",
    "1. Luego en 1997 Deep Blue venció a un campeón de ajedréz, Gary Kasparov en un juego de 6 partidas, esta IA examinaba 200 posibles posiciones por segundo a partir de cómo se encontraba la mesa. No está resuelto pues es posible jugar mejor que Deep blue (sin embargo se considera como un experto)\n",
    "\n",
    "1. En 2016 por primera vez se venció a un campeón de Go, Lee Sedol en un juego de 5 partidas, AlphaGo es el nombre de la IA que lo logró utiliza montecarlos así como una búsqueda por árboles y deep learning. También se considera como experta.\n",
    "\n",
    "1. ¿Pacman? Es de lo que se quiere averiguar en este vídeo.\n",
    "\n",
    "El reto que presenta este juego así como la Inteligencia artificial en general es como escribir código que genere comportamientos de cómo se esperajugar pero sin decirselo explícitamente (hard coding).\n",
    "\n",
    "Es importe tener en cuenta que lo que normalmente se analiza son los juegos de mesa o algún juego computacional, estos normalmente son determinísitcos, es que no hay probabilidad de que las movidas que hagamos fallen, no hay ningún elemento aleatorio en el juego.\n",
    "\n",
    "Un detalle interesante que menciona es que hay juegos con información perfecta, estos son en los que podemos ver todo el ambiente como lo es cualquier juego de mesa, sin embargo hay juegos con información imperfecta como un MOBA donde no logramos ver todo el ambiente porque nos debenmos mover para poder ver el resto del lugar donde sucede la \"acción\".\n",
    "\n",
    "En este tipo de algoritmo lo que se quiere es ir más allá de un plan, osea tener una estrategia, vamos a tener los siguientes factores.\n",
    "1. S: como conjunto de estados\n",
    "1. Players: como conjunto de 1 a n (usualmente toman turnos)\n",
    "1. Acciones: son dependientes del jugador y el estado.\n",
    "1. Función de transición: SxA -> S\n",
    "1. Terminal test: S->{t,f} (cómo termina el juego)\n",
    "1. Terminal Utilities: SxP -> R (Cuando encontramos el final del juego la idea es buscar uno favorable).\n",
    " \n",
    "#### Zero sum game\n",
    "En estos los agentes tienen utilidades opuestas, osea si digo que en ajedréz que al ganar tengo una utilidad de 1, el rival tendría una utilidad de -1, ambos 0 en caso de empate. Si sumamos \"mi\" utilidad y la del rival su suma da 0.\n",
    "\n",
    "Este modelo aplica a muchos juegos por lo que es muy útil y nos permite maximizar nuestra utiliad, para el caso del rival se debería buscar sería minimizarla.\n",
    "\n",
    "También existen juegos Zero Sum que implican más de 2 jugadores, el detalle es que no se puede aplicar la cualidad mencionada arriba que la utilidad del rival es la nuestra negada.\n",
    "\n",
    "#### Single-agent Trees\n",
    "\n",
    "Para estos árboles lo que normalmente se hace es que a partir de la posición original calculamos nuestros posibles movimientos hasta que lleguemos a un estado con el valos \"máximo\". El procesidimento inicia viendo las hojas, escogiendo el mayor beneficio de los hijos para colocárselo como valor al padre y así hacia arriba, con ello tenemos el valor real del árbol.\n",
    "\n",
    "#### Adversarial game Trees\n",
    "Aquí hay un fantasma para el juego de Pacman en el tablero, luego de que se haga una movida, el fantasma puede realiza otra, aquí se obtienen utilidades negativas pues hay casos donde Pacman muerte. \n",
    "\n",
    "Con este árbol podemos ver cuales son las posibilidades de lo que puede llegar a suceder pero realmente no se está seguro de qué evanto va a ocurrir, así que se necesita una noción para calculas el valor de las hojas, esto lo realizamos con los **minimax values**. \n",
    "\n",
    "Para esto el estado terminal no cambia, pero para hacer el cálculo de los padres, cada vez que es turno del oponente, en lugar de maximizar, ellos toman el resultado que va a minimizar mi puntaje, mientras sea nuestru turno se buscará maximizar.\n",
    "\n",
    "Con toda esta información explicada se realiza un ejemplo con el juego de \"gato\" o tik-tak-toe en inglés donde nos se puede visualizar más claramente el cómo se genera este árbol y cómo se le da el valor al mismo, eso si, desde una perspectiva más humana, con todo el árbol listo y buscando mínimos en el turno del rival y máximos en el nuestro.\n",
    "\n",
    "Luego vemos un ejemplo con un árbol cualquiera donde se resalta la importancia de cómo realiza realmente el cálculo el algoritmo recursivo, este funciona como un depth first y veficia los max-min dependiendo del  jugador.\n",
    "\n",
    "El detalle de este algoritmo es que tiene un tiempo de O(B^m) donde b es el factor de la rama y m es la total de movidas de profundidad en el juego; además a nivel de espacio tiene un O(bm).\n",
    "\n",
    "Esto trae un problema y es que en los juegos realísticos no se pueden alcanzar los nodos hoja, para ello la soluciónes el algoritmo de búsqueda **depth limited search** el cual realiza un búsqueda hasta cierta profundidad en el árbol. además cambiar las utilidades terminales con una función de evaluación para las posiciones no terminales.\n",
    "\n",
    "Con este algoritmo podemos garantizar que se estará realizando un juego óptimo aunque no se pueda ver la totalidad del tablero. En general, al tener más pliegues hay una gran diferencia en la precisión que tendremos a la hora de estimar el peso del árbol.\n",
    "\n",
    "#### **Note: Depth Matters**\n",
    "Las evaluaciones son siempre imperfectas, entre más adentro esté la función de evaluación, menos importa la calidad de esta. Es importante esa compensación entre la complejidad de las variables y la de la computación que implica trabajarla.\n",
    "\n",
    "### Funciones de evaluación\n",
    "\n",
    "Este tipo de funciones sirven para calcular puntaja de nodos no terminales en el Depth limited search, la función ideal retorna el actual minimax de la posición actual. En la práctica, se usa una suma lineal con pesos donde los pesos casi siempre osn números positivos que se multiplican por alguna función (En adejréz por ejemplo, el número de reinas que tengo - el número de reinas del rival, lo mismo con el resto de piesas)\n",
    "\n",
    "Estos pesos y funciones que se realizan para realizar el cómo se va a calcular la función de evaluación depende mucho del problema que estemos enfrentando, para el caso de Pacman podría ser qué tan cerca está de la comida o de un fantasma, etc.\n",
    "\n",
    "Comentan también que también lo mejor que se puede hacer para calcular los pesos claramente es trabajar con algún otro algoritmo de machine learning, no necesariamente tiene que ser una función lineal pero funciona bastante bien.\n",
    "\n",
    "A la hora de hacer los recortes de profundidad en el árbol puede que hayan ocaciones donde 2 desiciones sean igual de buenas y dependiendo de la acción se puedan enciclar, para evitar este tipo de situaciones podemos agregar algún factor que nos haga determinan cuál de las 2 es realmente mejor que la otra.\n",
    "\n",
    "Como ejemplo también se toma a Pacman, donde se tiene en consideración que la manera de lograr el desempate se da a partir de qué tan cerca está Pacman de la bolita que debe comer.\n",
    " \n",
    "### Tree pruning\n",
    "Aquí vemos una manera de podar el árbol a la hora de anlizar los pesos, el algoritmo consiste en ir realizando el minimax como lo conocemos, pero a la hora de ir haciéndo búsque en otros nodos verificamos si encontramos una hoja <= al mejor peso que tenemos de momento, en caso de que lo sea, sigue buscando en los hijos, sino desacha por completo esa rama del árbol. \n",
    "\n",
    "Este algoritmo se llama **Alpha-Beta pruning** y también considera la sección de minimización del rival a la hora de hacer los cálculos de los nodos no terminales.\n",
    "Además es útil aplicarselo a los nodos no terminales diferentes al root pues realmente no habría forma de mejorar al resultado de este, solo de sus hijos.\n",
    "\n",
    "**Fin del resúmen**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-crawford",
   "metadata": {},
   "source": [
    "### Basic Ensemble Learning (Random Forest, AdaBoost, Gradient Boosting)\n",
    "Es un modelo que hace predicciones basado en el número diferent de modelos. Combinando modelos individuales, se hace que este modelo sea más flaxible (menor Bias) y menos sensitivo a datos (Menor varianza).\n",
    "\n",
    "1. Bagging: este método entrena modelos individuales en paralelo. Cada modelo es entrenado con un subconjunto de datos aleatorio.\n",
    "\n",
    "1. Boosting: Se entrenan modelos individuales de manera secuencial. Cada modelo aprende de los errores cometidos por el modelo anterior.\n",
    "\n",
    "Como algoritmo de Bagging se puede usar el **Random Forest** para la sección de ensamble y los árboles de decisión como modelos individuales. Es decir que cada arbol se va a encargar a partir de un conjunto aleatorio de datos de \"votar\" y seleccionar por mayoría, cual es la respuesta correcta y luego entre todas las respuestas de los árboles se toma la que más se repita.\n",
    "\n",
    "**AdaBoost** es la variante con boosting que trabaja también con los árboles de desición. La llave para un buen funcionamiento es aprender de los fallos previos, en este caso incrementando el peso de los puntos de datos mal clasificados.\n",
    "\n",
    "**Gradient Boosting** es otro tipo de boosting que aprende de los fallos residuales, en lugar de actualizar los pesos de los puntos de datos.\n",
    "\n",
    "Para ejemplificar mejor el funcionamiento de cada uno, veremos como funciona estos algoritmos usando sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "every-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Library\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Step1: load the data set\n",
    "wineData = pd.read_csv(\"winequality-red.csv\")\n",
    "wineData['category'] = wineData['quality'] >= 7\n",
    "\n",
    "X = wineData[wineData.columns[0:11]].values\n",
    "y = wineData['category'].values.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vital-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2: Split the training test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tropical-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_stan = scaler.fit_transform(X_train)\n",
    "X_test_stan = scaler.transform(X_test) # don't forget this step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "soviet-companion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89375"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Fit a Decision Tree model as comparison\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train_stan, y_train)\n",
    "y_pred = clf.predict(X_test_stan)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "increasing-northwest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90625"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "# Step 4: Fit a Random Forest model\n",
    "clf = RandomForestClassifier(n_estimators=100, max_features=\"auto\",random_state=0)\n",
    "clf.fit(X_train_stan, y_train)\n",
    "y_pred = clf.predict(X_test_stan)\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incoming-issue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Fit a AdaBoost model\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "clf.fit(X_train_stan, y_train)\n",
    "y_pred = clf.predict(X_test_stan)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distributed-beast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88125"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Fit a Gradient Boosting model\n",
    "clf = GradientBoostingClassifier(n_estimators=100)\n",
    "clf.fit(X_train_stan, y_train)\n",
    "y_pred = clf.predict(X_test_stan)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
